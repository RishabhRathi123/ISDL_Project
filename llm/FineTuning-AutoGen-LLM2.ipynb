{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn9YdvHTwp02"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch accelerate evaluate wandb kaggle -q\n",
        "\n",
        "# Import Libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import evaluate\n",
        "import wandb\n",
        "from gemini_library import GeminiModel\n",
        "import pandas as pd\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Kaggle API authentication (if needed)\n",
        "os.environ['KAGGLE_USERNAME'] = 'rishabhrathi123'  # Replace with your Kaggle username\n",
        "os.environ['KAGGLE_KEY'] = 'adb0071fbff71b03dc3c895cefaceec4'  # Replace with your Kaggle API key\n",
        "\n",
        "# Step 1: Download Dataset\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "dataset_path = \"patzshane/football-commentary-data-set-college-and-nfl\"\n",
        "local_path = \"./football_commentary_dataset\"\n",
        "os.makedirs(local_path, exist_ok=True)\n",
        "api.dataset_download_files(dataset_path, path=local_path, unzip=True)\n",
        "\n",
        "# Load Dataset from Local Path\n",
        "dataset_file = os.path.join(local_path, \"data.csv\")  # Adjust if the file name differs\n",
        "data = pd.read_csv(dataset_file)\n",
        "print(data.head())  # Preview the dataset\n",
        "\n",
        "# Step 2: Convert Dataset to Hugging Face Format\n",
        "dataset = DatasetDict({\n",
        "    \"train\": load_dataset(\"csv\", data_files={\"train\": dataset_file})[\"train\"]\n",
        "})\n",
        "\n",
        "# Preview dataset\n",
        "print(dataset)\n",
        "\n",
        "# Step 3: Initialize Pre-Trained Model and Tokenizer\n",
        "model_name = \"gemini-1.5-flash\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Step 4: Tokenize Dataset with Dynamic Padding\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize based on the dialogue structure\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Use DataCollator for dynamic padding during training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Set to True for masked language modeling tasks\n",
        ")\n",
        "\n",
        "# Step 5: Set Up Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./autogen_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,  # Adjust for multiple agents\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,\n",
        "    push_to_hub=False,\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU is available\n",
        "    gradient_accumulation_steps=2,  # For better utilization of GPU with longer sequences\n",
        "    report_to=\"wandb\"  # Enable WandB logging\n",
        ")\n",
        "\n",
        "# Step 6: Initialize Evaluation Metrics\n",
        "perplexity = evaluate.load(\"perplexity\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_advanced_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    perplexity_score = perplexity.compute(predictions=predictions, references=labels)\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=labels)\n",
        "    rouge_score = rouge.compute(predictions=predictions, references=labels)\n",
        "    return {\n",
        "        \"perplexity\": perplexity_score[\"perplexity\"],\n",
        "        \"bleu\": bleu_score[\"bleu\"],\n",
        "        \"rouge\": rouge_score[\"rouge1\"],\n",
        "    }\n",
        "\n",
        "# Step 7: Initialize Trainer (Handling Agent Interaction in the Model)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"train\"],  # Use a proper eval split\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_advanced_metrics,\n",
        ")\n",
        "\n",
        "# Step 8: Fine-Tune Model (with Autogen-specific setup)\n",
        "trainer.train()\n",
        "\n",
        "# Step 9: Evaluate Model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n",
        "\n",
        "# Step 10: Save Fine-Tuned Model\n",
        "output_dir = \"./finetuned_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "# Step 11: Generate Text with Fine-Tuned Model (for Dialogue Simulation)\n",
        "prompt = \"In the field of artificial intelligence, one of the most exciting developments is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    }
  ]
}